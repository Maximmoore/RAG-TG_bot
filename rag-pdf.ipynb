{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b89c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.pipeline.standard_pdf_pipeline import StandardPdfPipeline\n",
    "from docling.backend.pypdfium2_backend import PyPdfiumDocumentBackend\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "\n",
    "import asyncio\n",
    "import logging\n",
    "from aiogram import Bot, Dispatcher, types\n",
    "from aiogram.filters import Command\n",
    "from aiogram.methods import DeleteWebhook\n",
    "from aiogram.types import Message\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import requests\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "\n",
    "model = \"deepseek/deepseek-v3-0324\"\n",
    "\n",
    "opts = PdfFormatOption(\n",
    "    pipeline_cls=StandardPdfPipeline,\n",
    "    backend=PyPdfiumDocumentBackend,\n",
    "    pipeline_options=PdfPipelineOptions(do_ocr=True)\n",
    ")\n",
    "converter = DocumentConverter(format_options={InputFormat.PDF: opts})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9659dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Открываем ПДФ файл, очищаем. \n",
    "\n",
    "def read_pdf_file(file_path):\n",
    "    result = converter.convert(file_path, raises_on_error=False)\n",
    "    document = result.document\n",
    "    text = document.export_to_markdown()    \n",
    "    text = re.sub(r\"[\\x00-\\x1F\\x7F-\\x9F]+\", \"\", text)            # Удалить квадраты и пустые знаки\n",
    "    print(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3472fb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Делим текст на куски.\n",
    "\n",
    "def text_to_chunk(text, chunk_size=250, overlap = 50):\n",
    "    # Разделим текст с сохранением структуры предложения.\n",
    "    sentences = text.replace('\\n', ' ').split('. ')\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_size = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Ставим точку если срезали на середине.\n",
    "        if not sentence.endswith('.'):\n",
    "            sentence += '.'\n",
    "\n",
    "        sentence_size = len(sentence)\n",
    "\n",
    "        # Добавление предложения больше размера чанка?\n",
    "        if current_size + sentence_size > chunk_size - overlap and current_chunk:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            current_size = sentence_size\n",
    "        else:\n",
    "            current_chunk.append(sentence)\n",
    "            current_size += sentence_size\n",
    "\n",
    "    # Добавляем элементы списка черех пробел\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    print(\"Чанк:\", chunks)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc8da19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запускаем работу БД.\n",
    "client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "sentence_transformer = embedding_functions.SentenceTransformerEmbeddingFunction(model_name = \"all-MiniLM-L6-v2\")\n",
    "collection = client.get_or_create_collection(name = \"condei_info\", embedding_function = sentence_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c0f94fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Главная функции. Обработка пдф документов из папки. Открытие, чтение, деление, внесение в БД.\n",
    "\n",
    "def process_document(file_path):\n",
    "    content = read_pdf_file(file_path)\n",
    "    chunks = text_to_chunk(content)\n",
    "    file_name = os.path.basename(file_path)\n",
    "    metadatas = [{\"Источник\": file_name, \"Чанк\": i} for i in range(len(chunks))]\n",
    "    ids = [f\"{file_name}_чанк_{i}\" for i in range(len(chunks))]\n",
    "    return ids, chunks, metadatas\n",
    "\n",
    "def add_to_collection(collection, ids, chunks, metadatas):\n",
    "    batch_size = 100                                      # Вносим одновременно 100 токенов    \n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        end_idx = min(i + batch_size, len(chunks))\n",
    "        collection.add(\n",
    "            documents = chunks[i:end_idx],\n",
    "            metadatas = metadatas[i:end_idx],\n",
    "            ids = ids[i:end_idx]\n",
    "        )\n",
    "\n",
    "def process_and_add_documents(collection, folder_path):    \n",
    "    files = [os.path.join(folder_path, file) \n",
    "             for file in os.listdir(folder_path) \n",
    "             if os.path.isfile(os.path.join(folder_path, file))]\n",
    "    \n",
    "    for file_path in files:\n",
    "        print(f\"Обрабатываю {os.path.basename(file_path)}...\")        \n",
    "        ids, chunks, metadatas = process_document(file_path)        \n",
    "        add_to_collection(collection, ids, chunks, metadatas)        \n",
    "        print(f\"Добавлено {len(chunks)} чанков в БД\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd3f28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запуск главной функции обработки всех .pdf документов в папке /data.\n",
    "\n",
    "process_and_add_documents(collection, \"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "465f59ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запуск семантического поиска и вывод чего нашли.\n",
    "\n",
    "def semantic_search(collection, query, n_results):\n",
    "    results = collection.query(\n",
    "        query_texts = [query],\n",
    "        n_results = n_results\n",
    "    )\n",
    "\n",
    "    # Просто нормально выводим итог семантического поиска\n",
    "    print(\"\\nРезультат поиска:\\n\" + \"-\" * 50)\n",
    "\n",
    "    for i in range(len(results['documents'][0])):\n",
    "        doc = results['documents'][0][i]\n",
    "        meta = results['metadatas'][0][i]\n",
    "        distance = results['distances'][0][i]\n",
    "        print(f\"\\nРезультат {i + 1}: Источник: {meta['Источник']}, Чанк {meta['Чанк']}, Точность {round(distance, 3)}\")\n",
    "        print(f\"Что нашли: {doc}\\n\")\n",
    "    print(\"-\" * 50)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "080178df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Фомируем информацию за запроса.\n",
    "\n",
    "def get_context_for_query(results):\n",
    "    context = \"\\n\\n\".join(results['documents'][0])\n",
    "    sources = [\n",
    "        f\"{meta['Источник']} (Чанк {meta['Чанк']})\"\n",
    "        for meta in results['metadatas'][0]\n",
    "    ]    \n",
    "    return context, sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52811700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подключаем ЛЛМ.\n",
    "\n",
    "API_URL = \"https://router.huggingface.co/novita/v3/openai/chat/completions\"\n",
    "headers = {\"Authorization\": f\"Bearer {os.environ['HF_TOKEN']}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3121bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========Добавляем возможность вести беседу=========\n",
    "\n",
    "conversations = {}  # Заменяем на БД\n",
    "\n",
    "def create_session():\n",
    "    session_id = str(uuid.uuid4())\n",
    "    conversations[session_id] = []\n",
    "    return session_id\n",
    "\n",
    "def add_message(session_id, role, content):\n",
    "    if session_id not in conversations:\n",
    "        conversations[session_id] = []\n",
    "\n",
    "    conversations[session_id].append({\n",
    "        \"role\": role,\n",
    "        \"content\": content,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36e567dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Извлекаем историю общения и форматируем её в удобную для промпта строку.\n",
    "\n",
    "def get_conversation_history(session_id, max_return_messages = None):\n",
    "    if session_id not in conversations:\n",
    "        return[]\n",
    "    history = conversations[session_id]\n",
    "    if max_return_messages:\n",
    "        history = history[-max_return_messages:]\n",
    "    return history\n",
    "\n",
    "def format_history_for_include_in_prompt(session_id, max_return_messages = 5):\n",
    "    history = get_conversation_history(session_id, max_return_messages)\n",
    "    formatted_history = \"\"\n",
    "\n",
    "    for msg in history:\n",
    "        role = \"user\" if msg[\"role\"] == \"user\" else \"Assistant\"\n",
    "        formatted_history += f\"{role}: {msg['content']}\\n\\n\"\n",
    "    print(\"Отформатированная история: \", formatted_history)\n",
    "    return formatted_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b257d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Учим модель понимать смысл уточняющих вопросов и дополнять контекст\n",
    "\n",
    "def contextualize_query(query, conversation_history):\n",
    "\n",
    "    # Создаём отдельную базу запросов, основанную на истории и последнем запросе пользователя. Шаблон.    \n",
    "    contextualize_q_system_prompt = (\n",
    "        \"Дано: история чата и последний вопрос пользователя, который может ссылаться на контекст из истории. Сформулируй самодостаточный (standalone) вопрос, не меняя названий, который будет понятен без истории чата. Не нужно отвечать на вопрос, только переформулируй его при необходимости. Если переформулировка не требуется — просто верни его как есть.\"\n",
    "    )    \n",
    "    payload = {\n",
    "    \"messages\": [\n",
    "        {   \"role\": \"system\", \n",
    "            \"content\": contextualize_q_system_prompt\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"История общения: {conversation_history} Вопрос: {query}\"\n",
    "        }\n",
    "    ],\n",
    "    \"model\": model,\n",
    "    \"temperature\": 0.1    \n",
    "    }  \n",
    "    \n",
    "    response = requests.post(API_URL, headers=headers, json=payload)    \n",
    "    print(\"Ответ json:\\n\", response.json())\n",
    "    inference = response.json()[\"choices\"][0][\"message\"]    \n",
    "    result = inference[\"content\"]    \n",
    "    return result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c389524c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получаем промпт с историей общения. \n",
    "\n",
    "def get_prompt(context, conversation_history, query):    \n",
    "    prompt = f\"\"\"Используй контекст из документа и истории разговора чтобы дать развёрнутый ответ, не меняя названий. Если контекста из документа не релевантный, используй только историю разговора. Если контекст из документа не релевантный и история разговора отсутствует, напиши: Недостаточно информации. Пожалуйста, уточните вопрос.\n",
    "    Контекст из документа:\n",
    "    {context}\n",
    "    История разговора:\n",
    "    {conversation_history}\n",
    "\n",
    "    Юзер: {query}\n",
    "\n",
    "    Агент:\"\"\"    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed9cfc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обновляем функцию ответа историей общения\n",
    "\n",
    "def generate_augmented_response(query, context, conversation_history = \"\"):    \n",
    "    prompt = get_prompt(context, conversation_history, query)    \n",
    "    payload = {\n",
    "    \"messages\": [\n",
    "        {   \"role\": \"system\", \n",
    "            \"content\": prompt},\n",
    "        \n",
    "    ],\n",
    "    \"model\": model,\n",
    "    \"temperature\": 0.1    \n",
    "    }  \n",
    "    response = requests.post(API_URL, headers=headers, json=payload)  \n",
    "    inference = response.json()[\"choices\"][0][\"message\"]\n",
    "    print(\"Инфиренс: \", response.json())\n",
    "    result = inference[\"content\"]    \n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0e99ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объединяем финальный пайплайн для рага\n",
    "\n",
    "def conversational_rag_query(\n",
    "        collection, \n",
    "        query,\n",
    "        session_id,\n",
    "        n_chunks = 3\n",
    "):\n",
    "    # Получаем историю в нужном формате.\n",
    "    conversation_history = format_history_for_include_in_prompt(session_id)    \n",
    "    \n",
    "    # Получаем историю в нужном формате.    \n",
    "    query = contextualize_query(query, conversation_history)\n",
    "    print(\"Запрос с контекстом: \", query)\n",
    "\n",
    "    # Забираем нужные чанки.\n",
    "    context, sources = get_context_for_query(\n",
    "        semantic_search(collection, query, n_chunks)\n",
    "    )      \n",
    "\n",
    "    response = generate_augmented_response(query, context, conversation_history)\n",
    "\n",
    "    # Добавляем в историю общения.\n",
    "    add_message(session_id, \"user\", query)\n",
    "    add_message(session_id, \"assistant\", response)\n",
    "\n",
    "    return response, sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78a7b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определяем сессию (новый пользователь или нет)\n",
    "\n",
    "session_id = create_session()\n",
    "print(session_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6df9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Делаем запрос руками, запускаем работу.\n",
    "query = \"Как включить продув испарителя на сплите dantex RK-24SVG\"\n",
    "response, sources = conversational_rag_query(collection, query, session_id)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b1512e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= Подключаем ТГ-бота ========\n",
    "TOKEN='*'\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "bot = Bot(TOKEN)\n",
    "dp = Dispatcher()\n",
    "\n",
    "# Обрабатываем команду старт\n",
    "@dp.message(Command(\"start\"))\n",
    "async def cmd_start(message: types.Message):\n",
    "    await message.answer(\"Здравия желаю! Бот-консультант по кондиционерам фирмы Daici&Dantex ждёт ваш запрос. Если ответ не приходит в течении 10 секунд, сервер отключен. Напишите автору — он починет.\", parse_mode= \"HTML\")\n",
    "\n",
    "# Обработчик текста\n",
    "@dp.message(lambda message: message.text)\n",
    "async def filter_messages(message: Message):\n",
    "    query = message.text\n",
    "    response, _ = conversational_rag_query(collection, query, session_id)\n",
    "    await message.answer(response)\n",
    "\n",
    "\n",
    "async def main():\n",
    "    await bot(DeleteWebhook(drop_pending_updates=True))\n",
    "    await dp.start_polling(bot)\n",
    "\n",
    "try:\n",
    "    loop = asyncio.get_running_loop()\n",
    "except RuntimeError:\n",
    "    loop = None\n",
    "\n",
    "if loop and loop.is_running():\n",
    "    if 'bot_task' in globals():\n",
    "        print(\"⛔ Останавливаю старый polling...\")\n",
    "        bot_task.cancel()\n",
    "\n",
    "    print(\"⚡ Event loop уже запущен, создаю задачу...\")\n",
    "    bot_task = loop.create_task(main())\n",
    "else:\n",
    "    # Обычный Python\n",
    "    asyncio.run(main())\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     try:\n",
    "#         loop = asyncio.get_running_loop()\n",
    "#     except RuntimeError:\n",
    "#         loop = None\n",
    "\n",
    "#     if loop and loop.is_running():\n",
    "#         # уже есть активный event loop (например, Jupyter)\n",
    "#         task = loop.create_task(main())\n",
    "#     else:\n",
    "#         asyncio.run(main()) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
